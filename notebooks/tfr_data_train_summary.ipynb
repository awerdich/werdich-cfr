{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics for the training, evaluation and testing data sets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/obi0/andreas/data/cfr/tfr_200617/cfr\n"
     ]
    }
   ],
   "source": [
    "dset='cfr'\n",
    "meta_date = '200617'\n",
    "cfr_data_root = os.path.normpath('/mnt/obi0/andreas/data/cfr')\n",
    "tfr_dir = os.path.join(cfr_data_root, 'tfr_'+meta_date, dset)\n",
    "\n",
    "meta_dir = os.path.join(cfr_data_root, 'metadata_'+meta_date)\n",
    "print(tfr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFR .parquet data files\n",
    "train_files = sorted(glob.glob(os.path.join(tfr_dir, dset+'_a4c_train_'+meta_date+'_*.parquet')))\n",
    "eval_files = sorted(glob.glob(os.path.join(tfr_dir, dset+'_a4c_eval_'+meta_date+'_*.parquet')))\n",
    "test_files = sorted(glob.glob(os.path.join(tfr_dir, dset+'_a4c_test_'+meta_date+'_*.parquet')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([pd.read_parquet(file) for file in train_files])\n",
    "eval_df = pd.concat([pd.read_parquet(file) for file in eval_files])\n",
    "test_df = pd.concat([pd.read_parquet(file) for file in test_files])\n",
    "df = pd.concat([train_df, eval_df, test_df], ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7445\n"
     ]
    }
   ],
   "source": [
    "print(len(df.filename.unique()))\n",
    "# Save all image data that was converted to TFR\n",
    "dataset_file_name = 'global_pet_echo_dataset_200617_.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rest_global_mbf', 'stress_global_mbf', 'global_cfr_calc', 'post-2018', 'tracer_obi', 'mrn', 'study', 'pet_date', 'echo_date', 'petmrn_identifier', 'days_post_pet', 'difference(days)', 'pet_measurement', 'filename', 'dir', 'datetime', 'fileid', 'institution', 'model', 'manufacturer', 'index', 'frame_time', 'number_of_frames', 'heart_rate', 'deltaX', 'deltaY', 'a2c', 'a2c_laocc', 'a2c_lvocc_s', 'a3c', 'a3c_laocc', 'a3c_lvocc_s', 'a4c', 'a4c_far', 'a4c_laocc', 'a4c_lvocc_s', 'a4c_rv', 'a4c_rv_laocc', 'a5c', 'apex', 'other', 'plax_far', 'plax_lac', 'plax_laz', 'plax_laz_ao', 'plax_plax', 'psax_avz', 'psax_az', 'psax_mv', 'psax_pap', 'rvinf', 'subcostal', 'suprasternal', 'max_view', 'sum_views', 'dset_mode', 'rate', 'im_array_shape'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mode: ['train']\n",
      "View        : ['a4c']\n",
      "Patients    : 1319\n",
      "PET  studies: 1375\n",
      "Echo studies: 1923\n",
      "Echo videos : 5830\n",
      "\n",
      "Dataset mode: ['eval']\n",
      "View        : ['a4c']\n",
      "Patients    : 140\n",
      "PET  studies: 143\n",
      "Echo studies: 203\n",
      "Echo videos : 588\n",
      "\n",
      "Dataset mode: ['test']\n",
      "View        : ['a4c']\n",
      "Patients    : 250\n",
      "PET  studies: 264\n",
      "Echo studies: 364\n",
      "Echo videos : 1027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_numbers(df):\n",
    "    print(f'Dataset mode: {list(df[\"dset_mode\"].unique())}')\n",
    "    print(f'View        : {list(df.max_view.unique())}')\n",
    "    print(f'Patients    : {len(df.mrn.unique())}')\n",
    "    print(f'PET  studies: {len((df.petmrn_identifier.unique()))}')\n",
    "    print(f'Echo studies: {len((df.study.unique()))}') \n",
    "    print(f'Echo videos : {len((df.filename.unique()))}')\n",
    "\n",
    "for df in [train_df, eval_df, test_df]:\n",
    "    print_numbers(df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall image stats\n",
    "dset = pd.concat([train_df, eval_df, test_df], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "# Add width and height of the images\n",
    "dset = dset.assign(width = dset.im_array_shape.apply(lambda s: s[1]),\n",
    "                   height = dset.im_array_shape.apply(lambda s: s[0]))\n",
    "\n",
    "# Let's define a scale factor column\n",
    "im_size = 299\n",
    "dset = dset.assign(sf = dset.im_array_shape.apply(lambda s: im_size/np.amax([s[0], s[1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum image height 298\n",
      "Maximum image width  398\n",
      "Image scale factor 0.7513\n"
     ]
    }
   ],
   "source": [
    "max_image_size = (dset.height.max(), dset.width.max())\n",
    "print('Maximum image height {}'.format(max_image_size[0]))\n",
    "print('Maximum image width  {}'.format(max_image_size[1]))\n",
    "image_scale_factor = 299/np.amax(max_image_size)\n",
    "print('Image scale factor {:.4f}'.format(image_scale_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of table: (7819, 60)\n",
      "Percentile boundaries: [1.154, 1.262, 1.391]\n"
     ]
    }
   ],
   "source": [
    "# Percentile the scale factors\n",
    "print('Size of table:', dset.shape)\n",
    "sf_array = dset.sf.values\n",
    "p_list = [np.round(np.percentile(sf_array, p), decimals = 3) for p in (25, 50, 75)]\n",
    "print('Percentile boundaries:', p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum image size that scales at 25th percentile: 259.0987868284229\n"
     ]
    }
   ],
   "source": [
    "print('Maximum image size that scales at 25th percentile: {}'.format(299/np.min(p_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
